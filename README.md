# Awesome-Egocentric-and-Exocentric-Vision
## Contents
- [Papers](#Papers)

    - [Egocentric for Exocentric](#Egocentric-for-Exocentric)
    - [Exocentric for Egocentric](#Exocentric-for-Egocentric)
    - [Joint Learning](#Joint-Learning)
- [Datasets](#Datasets)
## Papers

### Egocentric for Exocentric

#### Video Generation

- [Intention-driven Ego-to-Exo Video Generation](https://arxiv.org/abs/2403.09194)
- [Ego-to-Exo: Interfacing Third Person Visuals from Egocentric Views in Real-time for Improved ROV Teleoperation](https://arxiv.org/abs/2407.00848)

#### Action Understanding
- [From my view to yours: Egoaugmented learning in large vision language models for understanding exocentric daily living activities](https://arxiv.org/abs/2501.05711)

#### View Birdification
- [View birdification in the crowd: Ground-plane localization from perceived movements](https://arxiv.org/abs/2111.05060)
- [Viewbirdiformer: Learning to recover ground-plane crowd trajectories and ego-motion from a single ego-centric view](https://ieeexplore.ieee.org/document/9944870)
- [Incrowdformer: On-ground pedestrian world model from egocentric views](https://arxiv.org/abs/2303.09534)

#### Camera Registration
- [Relating view directions of complementary-view mobile cameras via the human shadow](https://dl.acm.org/doi/10.1007/s11263-022-01744-z)
- [From a bird’s eye view to see: Joint camera and subject registration without the camera calibration](https://ieeexplore.ieee.org/document/10657684)

#### Camera Selection
- [Camera selection for occlusion-less surgery recording via training with an egocentric camera](https://ieeexplore.ieee.org/document/9562527)

### Exocentric for Egocentric

#### Video Generation
- [Towards third-person visual imitation learning using generative adversarial networks](https://ieeexplore.ieee.org/document/9962214)
- [From third person to first person: Dataset and baselines for synthesis and retrieval](https://arxiv.org/abs/1812.00104)
- [Crossview exocentric to egocentric video synthesis](https://arxiv.org/abs/2107.03120)
- [Parallel generative adversarial network for third-person to first-person image generation](https://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Liu_Parallel_Generative_Adversarial_Network_for_Third-Person_to_First-Person_Image_Generation_CVPRW_2022_paper.pdf)
- [Cross-view exocentric to egocentric video synthesis](https://arxiv.org/abs/2107.03120)
- [4diff: 3d-aware diffusion model for third-to-first viewpoint translation](https://klauscc.github.io/4diff)
- [Diffusing in someone else’s shoes: Robotic perspective taking with diffusion](https://arxiv.org/abs/2404.07735)
- [Put myself in your shoes: Lifting the egocentric perspective from exocentric videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05558.pdf)


#### Video Captioning
- [Exo2egodvc: Dense video captioning of egocentric procedural activities using web instructional videos](https://arxiv.org/abs/2311.16444)
- [Retrieval-augmented egocentric video captioning](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Retrieval-Augmented_Egocentric_Video_Captioning_CVPR_2024_paper.pdf)

#### Action Understanding
- [Cross-view action recognition understanding from exocentric to egocentric perspective](https://arxiv.org/abs/2305.15699)
- [Unlocking exocentric video-language data for egocentric video representation learning](https://arxiv.org/abs/2408.03567)
- [Audio-Adaptive Activity Recognition Across Video Domains](https://arxiv.org/abs/2203.14240)
- [Cross-view generalisation in action recognition: Feature design for transitioning from exocentric to egocentric views](https://arxiv.org/abs/2305.15699)
- [Learning from semantic alignment between unpaired multiviews for egocentric video recognition](https://ieeexplore.ieee.org/abstract/document/10378238)
- [Estimating egocentric 3d human pose in the wild with external weak supervision](https://arxiv.org/abs/2201.07929)
- [Unsupervised and semi-supervised domain adaptation for action recognition from drones](https://ieeexplore.ieee.org/document/9093511)
- [Ego-exo: Transferring visual representations from third-person to first-person videos](https://arxiv.org/abs/2104.07905)
- [Synchronization is all you need: Exocentric-to-egocentric transfer for temporal action segmentation with unlabeled synchronized video pairs](https://arxiv.org/abs/2312.02638)
- [Egofish3d: Egocentric 3d pose estimation from a fisheye camera via selfsupervised learning](https://ieeexplore.ieee.org/document/10037218)
- [Ex2eg-mae: A framework for adaptation of exocentric video masked autoencoders for egocentric social role understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10301.pdf)

#### Affordance Grounding
- [Learning affordance grounding from exocentric images](https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Affordance_Grounding_From_Exocentric_Images_CVPR_2022_paper.pdf)
- [Locate: Localize and transfer object parts for weakly supervised affordance grounding](https://ieeexplore.ieee.org/document/10203835)
- [Weakly supervised multimodal affordance grounding for egocentric images](https://ojs.aaai.org/index.php/AAAI/article/view/28451)
- [Self-explainable affordance learning with embodied caption](https://arxiv.org/abs/2404.05603)
- [Strategies to leverage foundational model knowledge in object affordance grounding](https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Rai_Strategies_to_Leverage_Foundational_Model_Knowledge_in_Object_Affordance_Grounding_CVPRW_2024_paper.pdf)
- [Intra: Interaction relationship-aware weakly supervised affordance grounding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08064.pdf)
- [Learning granularity-aware affordances from humanobject interaction for tool-based functional grasping in dexterous robotics](https://arxiv.org/abs/2407.00614)

#### Remote Drone Teleoperation
- [Starhopper: A touch interface for remote object-centric drone navigation](https://graphicsinterface.org/proceedings/gi2020/gi2020-32/)
- [Droneaugmented human vision: Exocentric control for drones exploring hidden areas](https://ieeexplore.ieee.org/document/8260942)
- [Third-person piloting: Increasing situational awareness using a spatially coupled second drone](https://dl.acm.org/doi/10.1145/3332165.3347953)
- [Birdviewar: Surroundings-aware remote drone piloting using an augmented thirdperson perspective](https://dl.acm.org/doi/full/10.1145/3544548.3580681)

### Joint Learning
#### Video Generation
- [Cross-View Image Synthesis Using Conditional GANs](https://ieeexplore.ieee.org/document/8578467)

#### Video Captioning
- [Fourth-Person Captioning: Describing Daily Events by Uni-supervised and Tri-regularized Training](https://ieeexplore.ieee.org/document/8616361)
- [Lifelogging caption generation via fourth-person vision in a human–robot symbiotic environment](https://robomechjournal.springeropen.com/articles/10.1186/s40648-020-00181-2)

#### Cross-View Retrieval
- [Egotransfer: Transferring motion across egocentric and exocentric domains using deep neural networks](https://arxiv.org/abs/1612.05836)
- [From Third Person to First Person: Dataset and Baselines for Synthesis and Retrieval](https://arxiv.org/abs/1812.00104)
- [Actor and observer: Joint modeling of first and third-person videos](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf)
- [First- and third-person video co-analysis by learning spatial-temporal joint attention](https://ieeexplore.ieee.org/document/9220850)
- [Objectrelator: Enabling cross-view object relation understanding in ego-centric and exo-centric videos](https://arxiv.org/html/2411.19083v1)

#### Scene Mapping
- [Yowo: You only walk once to jointly map an indoor scene and register ceiling-mounted cameras](https://ieeexplore.ieee.org/document/10663468)

#### Action Understanding
Action Recognition
- [Action recognition in the presence of one egocentric and multiple static cameras](https://link.springer.com/chapter/10.1007/978-3-319-16814-2_12)
- [An exocentric look at egocentric actions and vice versa](https://arxiv.org/abs/1612.05836)
- [Actor and observer: Joint modeling of first and third-person videos](https://arxiv.org/abs/1804.09627)
- [Holistic-guided disentangled learning with cross-video semantics mining for concurrent first-person and third-person activity recognition](https://ieeexplore.ieee.org/document/9887964)
- [Recognizing micro-actions and reactions from paired egocentric videos](https://ieeexplore.ieee.org/document/7780657)
- [Look both ways: Self-supervising driver gaze estimation and road scene saliency](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730128.pdf)
- [Aide: A vision-driven multi-view, multi-modal, multitasking dataset for assistive driving perception](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf)
- [Holographic feature learning of egocentric-exocentric videos for multi-domain action recognition](https://ieeexplore.ieee.org/document/9429982)
- [Learning fine-grained view-invariant representations from unpaired ego-exo videos via temporal alignment](https://arxiv.org/abs/2306.05526)
- [Tell, don’t show: Language guidance eases transfer across domains in images and videos](https://arxiv.org/abs/2403.05535)
- [Pov: Prompt-oriented view-agnostic learning for egocentric hand-object interaction in the multi-view world](https://arxiv.org/abs/2403.05856)

Pose Estimation
- [Enhancing egocentric 3d pose estimation with third person views](https://arxiv.org/abs/2201.02017)
- [Next-generation surgical navigation: Marker-less multi-view 6dof pose estimation of surgical instruments](https://arxiv.org/abs/2305.03535)

#### Egocentric Wearer Identification
- [Ego2top: Matching viewers in egocentric and top-view videos](https://arxiv.org/abs/1607.06986)
- [Egocentric meets top-view](https://ieeexplore.ieee.org/document/8353133)
- [Integrating egocentric videos in top-view surveillance videos: Joint identification and temporal alignment](https://openaccess.thecvf.com/content_ECCV_2018/papers/Shervin_Ardeshir_Integrating_Egocentric_Videos_ECCV_2018_paper.pdf)
- [Joint person segmentation and identification in synchronized first- and third-person videos](https://openaccess.thecvf.com/content_ECCV_2018/papers/Mingze_Xu_Joint_Person_Segmentation_ECCV_2018_paper.pdf)
- [Fusing personal and environmental cues for identification and segmentation of first-person camera wearers in third-person views](https://ieeexplore.ieee.org/abstract/document/10656137)
- [Visual-gps: Ego-downward and ambient video based person location association](https://ieeexplore.ieee.org/document/9025474)
- [Seeing the unseen: Predicting the first-person camera wearer’s location and pose in third-person scenes](https://ieeexplore.ieee.org/document/9607539)
- [Identifying first-person camera wearers in third-person videos](https://openaccess.thecvf.com/content_cvpr_2017/papers/Fan_Identifying_First-Person_Camera_CVPR_2017_paper.pdf)

#### Cross-View Human Tracking and Association

- [Integrating egocentric videos in top-view surveillance videos: Joint identification and temporal alignment](https://openaccess.thecvf.com/content_ECCV_2018/papers/Shervin_Ardeshir_Integrating_Egocentric_Videos_ECCV_2018_paper.pdf)
- [Complementary-view co-interest person detection](https://dlnext.acm.org/doi/10.1145/3394171.3413659)
- [Complementary-view multiple human tracking](https://ojs.aaai.org/index.php/AAAI/article/view/6724)
- [Multiple human association and tracking from egocentric and complementary top views](https://ieeexplore.ieee.org/document/9394804)
- [Connecting the complementary-view videos: Joint camera identification and subject association](https://ieeexplore.ieee.org/document/9879989)

#### Robotic Manipulation
- [Vision-based manipulators need to also see from their hands](https://arxiv.org/abs/2203.12677)
- [Look closer: Bridging egocentric and third-person views with transformers for robotic manipulation](https://arxiv.org/abs/2201.07779)
- [Self-supervised disentangled representation learning for third-person imitation learning](https://ieeexplore.ieee.org/document/9636363)
- [Visual-policy learning through multi-camera view to single-camera view knowledge distillation for robot manipulation tasks](https://ieeexplore.ieee.org/document/10327777)
- [Multi-view disentanglement for reinforcement learning with multiple cameras](https://arxiv.org/abs/2404.14064)

#### Remote Drone Teleoperation
- [Spatial assisted human-drone collaborative navigation and interaction through immersive mixed reality](https://arxiv.org/abs/2402.04070)

#### Cross-View Geo-Localization
- [Learning deep representations for ground-to-aerial geolocalization](https://ieeexplore.ieee.org/document/7299135)
- [Localizing and orienting street views using overhead imagery](https://arxiv.org/abs/1608.00161)
- [Ground-to-aerial image geo-localization with a hard exemplar reweighting triplet loss](https://ieeexplore.ieee.org/document/9010868)
- [Netvlad: Cnn architecture for weakly supervised place recognition](https://ieeexplore.ieee.org/document/7937898)
- [Sample4geo: Hard negative sampling for cross-view geo-localisation](https://openaccess.thecvf.com/content/ICCV2023/papers/Deuser_Sample4Geo_Hard_Negative_Sampling_For_Cross-View_Geo-Localisation_ICCV_2023_paper.pdf)
- [Wide-area image geolocalization with aerial reference imagery](https://ieeexplore.ieee.org/document/7410808)
- [Cvm-net: Cross-view matching network for image-based ground-to-aerial geo-localization](https://ieeexplore.ieee.org/document/8578856)
- [Cross-view geo-localization with layerto-layer transformer](https://proceedings.neurips.cc/paper_files/paper/2021/file/f31b20466ae89669f9741e047487eb37-Paper.pdf)
- [Transformer-guided convolutional neural network for cross-view geolocalization](https://arxiv.org/abs/2204.09967)
- [Dehi: A decoupled hierarchical architecture for unaligned ground-to-aerial geo-localization](https://ieeexplore.ieee.org/document/10177194)
- [Cross-view geo-localization via learning disentangled geometric layout correspondence](https://ojs.aaai.org/index.php/AAAI/article/view/25457)
- [Mfrgn: Multi-scale feature representation generalization network for ground-to-aerial geolocalization](https://github.com/HUST-IAL/MFRGN)
- [Transgeo: Transformer is all you need for cross-view image geo-localization](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_TransGeo_Transformer_Is_All_You_Need_for_Cross-View_Image_Geo-Localization_CVPR_2022_paper.pdf)
- [University-1652: A multi-view multisource benchmark for drone-based geo-localization](https://arxiv.org/abs/2002.12186)
- [Each part matters: Local patterns facilitate cross-view geo-localization](https://ieeexplore.ieee.org/document/9360609)
- [Planet-photo geolocation with convolutional neural networks](https://arxiv.org/abs/1602.05314)
- [Vigor: Cross-view image geolocalization beyond one-to-one retrieval](https://ieeexplore.ieee.org/document/9578740)
- [Visual cross-view metric localization with dense uncertainty estimates](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990089.pdf)
- [Spatial-aware feature aggregation for image based cross-view geo-localization](https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf)
- [Where am i looking at? joint location and orientation estimation by cross-view matching](https://ieeexplore.ieee.org/document/9157033)
- [Accurate 3-dof camera geo-localization via ground-to-satellite image matching](https://arxiv.org/abs/2203.14148)
- [Bridging the domain gap for ground-to-aerial image matching](https://ieeexplore.ieee.org/document/9010757)
- [Coming down to earth: Satellite-to-street view synthesis for geo-localization](https://openaccess.thecvf.com/content/CVPR2021/papers/Toker_Coming_Down_to_Earth_Satellite-to-Street_View_Synthesis_for_Geo-Localization_CVPR_2021_paper.pdf)
- [Lending orientation to neural networks for crossview geo-localization](https://ieeexplore.ieee.org/document/8954224)
- [View consistent purification for accurate cross-view localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_View_Consistent_Purification_for_Accurate_Cross-View_Localization_ICCV_2023_paper.pdf)
- [Optimal feature transport for cross-view image geo-localization](https://arxiv.org/abs/1907.05021)
- [Learning cross-view visual geo-localization without ground truth](https://arxiv.org/abs/2403.12702)

## Datasets
### Action Understanding
- CMU-MMAC [Guide to the carnegie mellon university multimodal activity (cmu-mmac) database](https://kilthub.cmu.edu/articles/journal_contribution/Guide_to_the_Carnegie_Mellon_University_Multimodal_Activity_CMU-MMAC_Database/6555020/1?file=12037214)
- H2O [H2o: Two hands manipulating objects for first person interaction recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.pdf)
- Assembly101 [Assembly101: A large-scale multi-view video dataset for understanding procedural activities](https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf)
- ARCTIC [ARCTIC: A dataset for dexterous bimanual hand-object manipulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf)
- OAKINK2 [Oakink2: A dataset of bimanual hands-object manipulation in complex task completion](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhan_OAKINK2_A_Dataset_of_Bimanual_Hands-Object_Manipulation_in_Complex_Task_CVPR_2024_paper.pdf)
- Homage [Home action genome: Cooperative compositional action understanding](https://openaccess.thecvf.com/content/CVPR2021/papers/Rai_Home_Action_Genome_Cooperative_Compositional_Action_Understanding_CVPR_2021_paper.pdf)
- LEMMA [Lemma: A multiview dataset for le arning m ulti-agent m ulti-task a ctivities](https://arxiv.org/pdf/2007.15781)
- FT-HID [Ft-hid: a large-scale rgb-d dataset for first-and third-person human interaction analysis](https://arxiv.org/abs/2209.10155)
- EgoExo-Fitness [Egoexo-fitness: Towards egocentric and exocentric full-body action understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3057_ECCV_2024_paper.php)
- Charades-Ego [Actor and observer: Joint modeling of first and third-person videos](https://arxiv.org/abs/1804.09627)
- PolyU CFT [Holistic-guided disentangled learning with cross-video semantics mining for concurrent first-person and third-person activity recognition](https://ieeexplore.ieee.org/document/9887964)
- CORE4D [Core4d: A 4d humanobject-human interaction dataset for collaborative object rearrangement](https://arxiv.org/abs/2406.19353)
- Ego-Exo4D [Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives](https://ieeexplore.ieee.org/document/10658224)
- EgoExoLearn [Egoexolearn: A dataset for bridging asynchronous ego-and exo-centric view of procedural activities in real world](https://ieeexplore.ieee.org/document/10657292)
- EgoPW [Estimating egocentric 3d human pose in the wild with external weak supervision](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Estimating_Egocentric_3D_Human_Pose_in_the_Wild_With_External_CVPR_2022_paper.pdf)
- First2Third-Pose [Enhancing egocentric 3d pose estimation with third person views](https://arxiv.org/abs/2201.02017)
- ECHP [Egofish3d: Egocentric 3d pose estimation from a fisheye camera via selfsupervised learning](https://ieeexplore.ieee.org/document/10037218)
- AssemblyHands [Assemblyhands: Towards egocentric activity understanding via 3d hand pose estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf)
- ThermoHands [Thermohands: A benchmark for 3d hand pose estimation from egocentric thermal image](https://arxiv.org/abs/2403.09871)
- EgoHumans [Ego-humans: An ego-centric 3d multi-human benchmark](https://ieeexplore.ieee.org/document/10377933)
- OVR [Ovr: A dataset for open vocabulary temporal repetition counting in videos](https://arxiv.org/abs/2407.17085)
- [Next-generation surgical navigation: Marker-less multi-view 6dof pose estimation of surgical instruments](https://arxiv.org/abs/2305.03535)

### Driving
- LBW [Look both ways: Self-supervising driver gaze estimation and road scene saliency](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730128.pdf)
- AIDE [Aide: A vision-driven multi-view, multi-modal, multitasking dataset for assistive driving perception](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf)
- WTS [Wts: A pedestrian-centric traffic video dataset for fine-grained spatial-temporal understanding](https://arxiv.org/html/2407.15350v1)

### Affordance Grounding
- AGD20K [Learning affordance grounding from exocentric images](https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Affordance_Grounding_From_Exocentric_Images_CVPR_2022_paper.pdf)
- FAH [Learning granularity-aware affordances from humanobject interaction for tool-based functional grasping in dexterous robotics](https://arxiv.org/abs/2407.00614)
- PAD [One-shot affordance detection](https://arxiv.org/abs/2106.14747)

### Generation
- ThirdtoFirst [Ego-exo: Transferring visual representations from third-person to first-person videos](https://arxiv.org/abs/2104.07905)

### Scene Understanding
- 360 + x [360 + x: A panoptic multi-modal scene understanding dataset](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_360x_A_Panoptic_Multi-modal_Scene_Understanding_Dataset_CVPR_2024_paper.pdf)

### Video Question Answering
- GazeVQA [Gazevqa: A video question answering dataset for multiview eye-gaze task-oriented collaborations](https://github.com/mfurkanilaslan/GazeVQA)

### Egocentric Wearer Identification
- Ego2Top [Ego2top: Matching viewers in egocentric and top-view videos](https://arxiv.org/abs/1607.06986)
- IUShareView [Identifying first-person camera wearers in third-person videos](https://openaccess.thecvf.com/content_cvpr_2017/papers/Fan_Identifying_First-Person_Camera_CVPR_2017_paper.pdf)
- TF2023 [Fusing personal and environmental cues for identification and segmentation of first-person camera wearers in third-person views](https://ieeexplore.ieee.org/abstract/document/10656137)

### Cross-View Human Tracking and Association
- CVMHT [Complementary-view multiple human tracking](https://ojs.aaai.org/index.php/AAAI/article/view/6724)
- DMHA [Connecting the complementary-view videos: Joint camera identification and subject association](https://ieeexplore.ieee.org/document/9879989)

### Camera Registration
- [From a bird’s eye view to see: Joint camera and subject registration without the camera calibration](https://ieeexplore.ieee.org/document/10657684)
- [Yowo: You only walk once to jointly map an indoor scene and register ceiling-mounted cameras](https://ieeexplore.ieee.org/document/10663468)

### Cross-View Geo-Localization
- [Learning deep representations for ground-to-aerial geolocalization](https://ieeexplore.ieee.org/document/7299135)
- VO [Localizing and orienting street views using overhead imagery](https://arxiv.org/abs/1608.00161)
- CVUSA [Wide-area image geolocalization with aerial reference imagery](https://ieeexplore.ieee.org/document/7410808)
- University-1652 [University-1652: A multi-view multisource benchmark for drone-based geo-localization](https://arxiv.org/abs/2002.12186)
- VIGOR [Vigor: Cross-view image geolocalization beyond one-to-one retrieval](https://ieeexplore.ieee.org/document/9578740)
- CVACT [Lending orientation to neural networks for crossview geo-localization](https://ieeexplore.ieee.org/document/8954224)
- CVGlobal [Cross-view image geo-localization with panorama-bev co-retrieval network](https://arxiv.org/abs/2408.05475)
- SUES-200 [Sues-200: A multi-height multi-scene cross-view image benchmark across drone and satellite](https://ieeexplore.ieee.org/document/10054158)




